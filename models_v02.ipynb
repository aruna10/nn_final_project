{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from math import sqrt\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes in training data: 2647\n",
      "Number of classes in testing data: 1\n",
      "Total number of classes: 2647\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "train_folder = \"H:/BIG DATA/nn_final_project/dataset/preprocessed-boneage-training-dataset\"\n",
    "test_folder = \"H:/BIG DATA/nn_final_project/dataset/boneage-test-dataset\"\n",
    "\n",
    "# Count the number of folders in the train and test directories\n",
    "num_classes_train = len(os.listdir(train_folder))\n",
    "num_classes_test = len(os.listdir(test_folder))\n",
    "\n",
    "print(\"Number of classes in training data:\", num_classes_train)\n",
    "print(\"Number of classes in testing data:\", num_classes_test)\n",
    "\n",
    "# Choose the maximum number of classes between train and test data\n",
    "num_classes = max(num_classes_train, num_classes_test)\n",
    "print(\"Total number of classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.4862745  0.4862745  0.4862745  ... 0.9019608  0.9843137  0.99607843]\n",
      "  [0.4862745  0.4862745  0.4862745  ... 0.9019608  0.9843137  0.99607843]\n",
      "  [0.4862745  0.4862745  0.4862745  ... 0.9019608  0.9843137  0.99607843]\n",
      "  ...\n",
      "  [0.73333335 0.73333335 0.73333335 ... 0.73333335 0.73333335 0.73333335]\n",
      "  [0.73333335 0.73333335 0.73333335 ... 0.8352941  0.8392157  0.8392157 ]\n",
      "  [0.73333335 0.73333335 0.73333335 ... 0.99215686 0.99215686 0.99215686]]\n",
      "\n",
      " [[0.21960784 0.21960784 0.21960784 ... 0.         0.         0.        ]\n",
      "  [0.21960784 0.21960784 0.21960784 ... 0.         0.         0.        ]\n",
      "  [0.21960784 0.21960784 0.21960784 ... 0.         0.         0.        ]\n",
      "  ...\n",
      "  [0.4        0.4        0.4        ... 0.         0.         0.        ]\n",
      "  [0.4        0.4        0.4        ... 0.         0.         0.        ]\n",
      "  [0.4        0.4        0.4        ... 0.         0.         0.        ]]\n",
      "\n",
      " [[0.8039216  0.8039216  0.79607844 ... 0.59607846 0.59607846 0.60784316]\n",
      "  [0.73333335 0.73333335 0.73333335 ... 0.5686275  0.5686275  0.58431375]\n",
      "  [0.6666667  0.6666667  0.6666667  ... 0.53333336 0.53333336 0.5529412 ]\n",
      "  ...\n",
      "  [0.69803923 0.6666667  0.63529414 ... 0.61960787 0.61960787 0.61960787]\n",
      "  [0.69803923 0.6666667  0.63529414 ... 0.61960787 0.61960787 0.61960787]\n",
      "  [0.7019608  0.6666667  0.63529414 ... 0.62352943 0.62352943 0.62352943]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.         0.         0.         ... 0.1254902  0.1254902  0.1254902 ]\n",
      "  [0.         0.         0.         ... 0.21176471 0.21176471 0.21176471]\n",
      "  [0.         0.         0.         ... 0.21176471 0.21176471 0.21176471]\n",
      "  ...\n",
      "  [0.654902   0.654902   0.6509804  ... 0.1254902  0.00784314 0.00392157]\n",
      "  [0.654902   0.654902   0.654902   ... 0.1254902  0.00784314 0.00392157]\n",
      "  [0.63529414 0.63529414 0.63529414 ... 0.1254902  0.00784314 0.00392157]]\n",
      "\n",
      " [[0.         0.9098039  0.8862745  ... 0.9843137  0.99215686 0.99607843]\n",
      "  [0.         0.8901961  0.85882354 ... 0.92156863 0.96862745 0.99607843]\n",
      "  [0.         0.8666667  0.8117647  ... 0.8627451  0.9098039  0.99215686]\n",
      "  ...\n",
      "  [0.8039216  0.78039217 0.7529412  ... 0.8        0.89411765 0.99607843]\n",
      "  [0.8039216  0.78039217 0.7529412  ... 0.8        0.89411765 0.99607843]\n",
      "  [0.80784315 0.7882353  0.75686276 ... 0.8        0.8980392  0.99607843]]\n",
      "\n",
      " [[0.79607844 0.79607844 0.79607844 ... 0.4392157  0.4392157  0.4392157 ]\n",
      "  [0.79607844 0.79607844 0.79607844 ... 0.4392157  0.4392157  0.4392157 ]\n",
      "  [0.79607844 0.79607844 0.79607844 ... 0.4392157  0.4392157  0.4392157 ]\n",
      "  ...\n",
      "  [0.79607844 0.79607844 0.79607844 ... 0.54509807 0.54509807 0.54509807]\n",
      "  [0.79607844 0.79607844 0.79607844 ... 0.54509807 0.54509807 0.54509807]\n",
      "  [0.79607844 0.79607844 0.79607844 ... 0.54509807 0.54509807 0.54509807]]]\n",
      "[ 96 168 168 ... 168 150 144]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# Load the metadata from the CSV file\n",
    "csv_file_path = \"H:/BIG DATA/nn_final_project/dataset/boneage-training-dataset.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Path to the folder containing the training images\n",
    "train_folder = \"H:/BIG DATA/nn_final_project/dataset/preprocessed-boneage-training-dataset\"\n",
    "\n",
    "# Lists to store the image data and bone age labels\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "# Loop through the rows in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    image_id = row['id']\n",
    "    image_filename = f\"{image_id}.png\"  # Assuming the image filename is stored as an integer 'id'\n",
    "    bone_age = row['boneage']\n",
    "    image_path = os.path.join(train_folder, image_filename)\n",
    "    if os.path.isfile(image_path):\n",
    "        try:\n",
    "            # Open and preprocess the image\n",
    "            img = Image.open(image_path)\n",
    "            # Preprocess the image as needed (e.g., resize, normalization)\n",
    "            img = np.array(img)\n",
    "            img = img.astype(\"float32\") / 255.0\n",
    "            # Add the preprocessed image and bone age label to the lists\n",
    "            x_train.append(img)\n",
    "            y_train.append(bone_age)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image: {image_filename}, Error: {e}\")\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "print(x_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\BIG DATA\\nn_final_project\\venv\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 882ms/step - loss: 7550.3496 - val_loss: 1804.5425\n",
      "Epoch 2/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 855ms/step - loss: 1472.9541 - val_loss: 1783.1729\n",
      "Epoch 3/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 1s/step - loss: 1566.6573 - val_loss: 1799.9706\n",
      "Epoch 4/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 1s/step - loss: 1569.0623 - val_loss: 1941.1333\n",
      "Epoch 5/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 937ms/step - loss: 1535.4979 - val_loss: 1856.0170\n",
      "Epoch 6/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 1s/step - loss: 1478.4456 - val_loss: 1766.1206\n",
      "Epoch 7/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 971ms/step - loss: 1597.7880 - val_loss: 1754.5330\n",
      "Epoch 8/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 1s/step - loss: 1602.3843 - val_loss: 1752.8374\n",
      "Epoch 9/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 1s/step - loss: 1551.7172 - val_loss: 1786.0359\n",
      "Epoch 10/10\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1s/step - loss: 1531.6990 - val_loss: 1749.1874\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 272ms/step\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 137ms/step\n",
      "Validation SSR: 122833786.73431134\n",
      "Validation MSE: 1749.1874804800116\n",
      "Validation RMSE: 41.8232887334319\n",
      "Training SSR: 8728085523.041979\n",
      "Training MSE: 1536.3113416746241\n",
      "Training RMSE: 39.19580770534808\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Normalize the images to [0, 1]\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "\n",
    "# Reshape the data to match the CNN input shape (assuming grayscale images)\n",
    "input_shape = x_train[0].shape + (1,)\n",
    "x_train = x_train.reshape(-1, *input_shape)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Build your CNN model architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))  # Output layer for regression (bone age)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model with the validation data\n",
    "history = model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_val, y_val))\n",
    "\n",
    "# Predict bone age labels for the training data\n",
    "y_train_pred = model.predict(x_train)\n",
    "\n",
    "# Calculate SSR (Sum of Squared Residuals) on training data\n",
    "ssr_train = np.sum((y_train_pred - y_train)**2)\n",
    "\n",
    "# Calculate MSE (Mean Squared Error) on training data\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "\n",
    "# Calculate RMSE (Root Mean Squared Error) on training data\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "\n",
    "# Calculate R-squared on training data\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "y_val_pred = model.predict(x_val)\n",
    "\n",
    "# Calculate SSR (Sum of Squared Residuals) on validation data\n",
    "ssr_val = np.sum((y_val_pred - y_val)**2)\n",
    "\n",
    "# Calculate MSE (Mean Squared Error) on validation data\n",
    "mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "\n",
    "# Calculate RMSE (Root Mean Squared Error) on validation data\n",
    "rmse_val = np.sqrt(mse_val)\n",
    "\n",
    "# Calculate R-squared on validation data\n",
    "r2_val = r2_score(y_val, y_val_pred)\n",
    "\n",
    "# Print validation metrics\n",
    "print(\"Validation SSR:\", ssr_val)\n",
    "print(\"Validation MSE:\", mse_val)\n",
    "print(\"Validation RMSE:\", rmse_val)\n",
    "\n",
    "# Print training metrics\n",
    "print(\"Training SSR:\", ssr_train)\n",
    "print(\"Training MSE:\", mse_train)\n",
    "print(\"Training RMSE:\", rmse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example of saving the model to a file using pickle\n",
    "model_file = \"model.pkl\"\n",
    "file_path = 'H:/BIG DATA/nn_final_project'\n",
    "# Define a function to save the model\n",
    "def save_model_to_pickle(model, file_path):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "# Save the model\n",
    "save_model_to_pickle(model, model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
